---
title: "Causal Compression Model Demonstrations"
format:
  html:
    code-fold: false
    toc: true
jupyter: python3
---

This notebook demonstrates the key concepts from the causal compression framework for speaker-listener communication.

## Key Predictions

1. **Context-Dependent Compression**: The same true causal model produces different optimal utterances depending on context
2. **Listener Trust Dynamics**: Same observation (advice revision) produces *opposite* trust updates depending on prior beliefs
3. **Rate-Distortion Trade-off**: Speakers balance compression (rate) against accuracy (distortion)

```{python}
import sys
sys.path.insert(0, '../src')

import numpy as np
import matplotlib.pyplot as plt

from causal_compression import (
    Variable, CausalDAG, Utterance,
    CompressionSpeaker, TrustModel,
    compute_rate_distortion_curve, plot_rate_distortion_curve,
    build_drug_marker_scenario
)
```

## 1. Context-Dependent Compression

**Scenario**: A doctor advises patients about a drug treatment.

- The drug's effectiveness depends on a genetic marker (G)
- G=1 (has marker): Drug is highly effective (90% success)
- G=0 (no marker): Drug is ineffective (20% success)

**Key prediction**: The same true DAG leads to different optimal utterances for different patients.

```{python}
# Build the scenario
true_dag, utterances = build_drug_marker_scenario()

# Create speaker model
speaker = CompressionSpeaker(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    alpha=10.0  # Higher alpha = more rational/optimal
)

print("True DAG structure: G (genetic marker) -> Y <- D (drug)")
print(f"Available utterances: {[u.name for u in utterances]}")
```

### Context 1: Patient HAS the genetic marker

```{python}
context_marker = {'G': 1}
losses_marker = speaker.compute_losses(context_marker)
probs_marker = speaker.get_utterance_probs(context_marker)
optimal_marker = speaker.get_optimal_utterance(context_marker)

print("Context: Patient HAS genetic marker (G=1)")
print("=" * 50)
print("\nInformation losses (KL divergence):")
for u, l in losses_marker.items():
    print(f"  {u}: {l:.4f} bits")
print("\nUtterance probabilities:")
for u, p in probs_marker.items():
    print(f"  {u}: {p:.4f}")
print(f"\n-> Optimal utterance: '{optimal_marker.name}'")
```

### Context 2: Patient LACKS the genetic marker

```{python}
context_no_marker = {'G': 0}
losses_no_marker = speaker.compute_losses(context_no_marker)
probs_no_marker = speaker.get_utterance_probs(context_no_marker)
optimal_no_marker = speaker.get_optimal_utterance(context_no_marker)

print("Context: Patient LACKS genetic marker (G=0)")
print("=" * 50)
print("\nInformation losses (KL divergence):")
for u, l in losses_no_marker.items():
    print(f"  {u}: {l:.4f} bits")
print("\nUtterance probabilities:")
for u, p in probs_no_marker.items():
    print(f"  {u}: {p:.4f}")
print(f"\n-> Optimal utterance: '{optimal_no_marker.name}'")
```

### Visualization

```{python}
#| fig-cap: "Context-dependent compression: same true DAG produces different optimal utterances"
#| label: fig-context-compression

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Utterance probabilities for each context
utterance_names = list(probs_marker.keys())
x = np.arange(len(utterance_names))
width = 0.35

ax = axes[0]
ax.bar(x - width/2, [probs_marker[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [probs_no_marker[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('P(utterance | context)')
ax.set_title('Utterance Probabilities by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

# Information loss for each context
ax = axes[1]
losses_marker_finite = {k: v if v < 10 else 10 for k, v in losses_marker.items()}
losses_no_marker_finite = {k: v if v < 10 else 10 for k, v in losses_no_marker.items()}

ax.bar(x - width/2, [losses_marker_finite[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [losses_no_marker_finite[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('Information Loss (bits)')
ax.set_title('Information Loss by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

::: {.callout-important}
## Key Result: Context-Dependent Compression

Same true DAG, but context changes optimal utterance!

- **G=1 (has marker)**: Speaker says `{python} optimal_marker.name`
- **G=0 (no marker)**: Speaker says `{python} optimal_no_marker.name`

This is RATIONAL context-dependent compression, not inconsistency.
:::

## 2. Listener Trust Dynamics

When a listener observes advice revision (e.g., doctor says "drug works" to one patient, then "drug doesn't work" to another), their trust update depends on their prior beliefs about world complexity.

**Key insight**: The same observation produces *opposite* trust updates!

- Listener who believes world is **simple**: Trust *decreases* ("Doctor is contradicting themselves!")
- Listener who believes world is **complex**: Trust *increases* ("Doctor is appropriately adapting to context!")

```{python}
# Create two listeners with different priors
listener_simple = TrustModel.create_simple_believer(reliability_prior=0.8)
listener_complex = TrustModel.create_complex_believer(reliability_prior=0.8)

print("Listener A (Simple Believer)")
print(f"  Prior P(world=simple) = 0.9")
print(f"  Prior P(speaker=reliable) = {listener_simple.get_reliability_belief():.2f}")

print("\nListener B (Complex Believer)")
print(f"  Prior P(world=complex) = 0.9")
print(f"  Prior P(speaker=reliable) = {listener_complex.get_reliability_belief():.2f}")
```

### Observing Advice Revision

```{python}
print("Observation: Doctor revises advice")
print("  T1: 'Drug works' (to patient with marker)")
print("  T2: 'Drug doesn't work' (to patient without marker)")
print("\n" + "=" * 60)

# Update Listener A (simple believer)
result_simple = listener_simple.update('revision')
print("\nListener A (believes world is SIMPLE):")
print(f"  Trust: {result_simple['prior_reliable']:.2f} -> {result_simple['posterior_reliable']:.3f}")
print(f"  Trust delta: {result_simple['trust_delta']:+.3f}")
if result_simple['trust_delta'] < 0:
    print("  -> TRUST DECREASED: 'Doctor is contradicting themselves!'")

# Update Listener B (complex believer)
result_complex = listener_complex.update('revision')
print("\nListener B (believes world is COMPLEX):")
print(f"  Trust: {result_complex['prior_reliable']:.2f} -> {result_complex['posterior_reliable']:.3f}")
print(f"  Trust delta: {result_complex['trust_delta']:+.3f}")
if result_complex['trust_delta'] > 0:
    print("  -> TRUST INCREASED: 'Doctor is adapting to context!'")
```

### Visualization

```{python}
#| fig-cap: "Opposite trust updates from the same observation"
#| label: fig-trust-dynamics

fig, ax = plt.subplots(figsize=(8, 5))

# Data
labels = ['Simple Believer', 'Complex Believer']
prior_trust = [result_simple['prior_reliable'], result_complex['prior_reliable']]
posterior_trust = [result_simple['posterior_reliable'], result_complex['posterior_reliable']]

x = np.arange(len(labels))
width = 0.35

bars1 = ax.bar(x - width/2, prior_trust, width, label='Before Revision',
               color='lightblue', edgecolor='steelblue')
bars2 = ax.bar(x + width/2, posterior_trust, width, label='After Revision',
               color='steelblue', edgecolor='darkblue')

# Add annotations showing direction of change
for i, (p1, p2) in enumerate(zip(prior_trust, posterior_trust)):
    mid_x = i
    color = 'red' if p2 < p1 else 'green'
    arrow_text = f"{p2-p1:+.3f}"
    ax.annotate(arrow_text, xy=(mid_x, max(p1, p2) + 0.05), ha='center', fontsize=12,
                color=color, fontweight='bold')

ax.set_ylabel('P(speaker is reliable)', fontsize=12)
ax.set_title('Trust Updates After Observing Advice Revision', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(labels, fontsize=11)
ax.legend(loc='lower right')
ax.set_ylim(0, 1.1)
ax.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

::: {.callout-important}
## Key Result: Opposite Trust Updates

Same observation (advice revision) produces opposite trust updates:

| Listener | Prior | Posterior | Change |
|----------|-------|-----------|--------|
| Simple Believer | 0.80 | `{python} f"{result_simple['posterior_reliable']:.3f}"` | `{python} f"{result_simple['trust_delta']:+.3f}"` |
| Complex Believer | 0.80 | `{python} f"{result_complex['posterior_reliable']:.3f}"` | `{python} f"{result_complex['trust_delta']:+.3f}"` |

This is **rational Bayesian inference**, not motivated reasoning. Both listeners are correctly updating given their priors.

**Practical implication**: Before giving context-dependent advice, help listeners understand that the domain IS context-dependent. Otherwise, your nuanced expertise will look like inconsistency.
:::

## 3. Rate-Distortion Trade-off

Following Zaslavsky et al. (2020), speakers trade off:

- **Rate** (complexity): Entropy of utterance distribution (bits)
- **Distortion**: Expected information loss (bits)

The rationality parameter $\alpha$ controls this trade-off:

- $\alpha \to 0$: Uniform distribution (low rate, high distortion)
- $\alpha \to \infty$: Deterministic optimal (high rate, low distortion)

```{python}
# Define contexts to average over
contexts = [{'G': 0}, {'G': 1}]

# Compute rate-distortion curve
rd_data = compute_rate_distortion_curve(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    contexts=contexts,
    alpha_range=np.logspace(-1, 2, 50)
)

print(f"Computed RD curve for {len(rd_data['alpha'])} alpha values")
print(f"Rate range: [{rd_data['rate'].min():.3f}, {rd_data['rate'].max():.3f}] bits")
print(f"Distortion range: [{rd_data['distortion'].min():.3f}, {rd_data['distortion'].max():.3f}] bits")
```

```{python}
#| fig-cap: "Rate-Distortion trade-off curve"
#| label: fig-rate-distortion

fig, axes = plot_rate_distortion_curve(rd_data, figsize=(14, 4))
plt.show()
```

**Interpretation**:

- **Low $\alpha$ (left)**: Speaker uses all utterances equally → high distortion, low complexity
- **High $\alpha$ (right)**: Speaker always picks optimal utterance → low distortion, high complexity
- The RD curve shows the fundamental trade-off between these objectives

## Summary

This framework provides a principled account of:

1. **Why speakers simplify**: Compression is rational when it preserves decision-relevant information in context

2. **Why advice changes**: Context-dependent compression means the same true model produces different optimal utterances

3. **Why trust varies**: Listeners' complexity priors determine whether revision signals reliability or inconsistency

4. **The efficiency trade-off**: Speakers operate along the rate-distortion frontier, balancing simplicity and accuracy
