---
title: "Causal Compression Demonstrations"
format:
  html:
    code-fold: true
    toc: true
jupyter: python3
---

## When does simplifying causal knowledge backfire?

Experts compress complex causal knowledge into simple advice. A doctor who knows a drug's effectiveness depends on a genetic marker might just say "take the drug" or "don't take the drug." This compression is *context-dependent*: the same expert gives different advice to different patients, not because they're unreliable, but because the underlying causal structure demands it.

The problem: a listener who doesn't know the causal structure might interpret context-sensitive advice as a sign the speaker is confused, unreliable, or manipulative. Whether compression backfires depends on:

1. **Complexity prior** — does the listener expect the world to be simple or complex?
2. **Vigilance** — does the listener entertain non-informative speaker types (persuasive, unreliable)?

```{python}
import sys
sys.path.insert(0, '..')

import numpy as np
import matplotlib.pyplot as plt

from models import (
    Variable, CausalDAG, Utterance,
    CompressionSpeaker,
    RSATrustModel, Goal,
    compute_rate_distortion_curve,
    compute_trust_curve,
    build_drug_marker_scenario,
    build_trust_update_scenario,
)
```

## The Speaker's Compression Problem

A drug's effectiveness depends on a genetic marker G. G=1: 90% success. G=0: 20% success. The speaker must compress this into one of two utterances.

```{python}
true_dag, utterances = build_drug_marker_scenario()

speaker = CompressionSpeaker(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    alpha=10.0
)

context_marker = {'G': 1}
context_no_marker = {'G': 0}

losses_marker = speaker.compute_losses(context_marker)
probs_marker = speaker.get_utterance_probs(context_marker)

losses_no_marker = speaker.compute_losses(context_no_marker)
probs_no_marker = speaker.get_utterance_probs(context_no_marker)

print(f"G=1 → optimal: '{speaker.get_optimal_utterance(context_marker).name}'")
print(f"G=0 → optimal: '{speaker.get_optimal_utterance(context_no_marker).name}'")
```

```{python}
#| fig-cap: "Utterance probabilities and information loss by context"
#| label: fig-context-compression

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

utterance_names = list(probs_marker.keys())
x = np.arange(len(utterance_names))
width = 0.35

ax = axes[0]
ax.bar(x - width/2, [probs_marker[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [probs_no_marker[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('P(utterance | context)')
ax.set_title('Utterance Probabilities by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

ax = axes[1]
losses_marker_finite = {k: v if v < 10 else 10 for k, v in losses_marker.items()}
losses_no_marker_finite = {k: v if v < 10 else 10 for k, v in losses_no_marker.items()}

ax.bar(x - width/2, [losses_marker_finite[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [losses_no_marker_finite[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('Information Loss (bits)')
ax.set_title('Information Loss by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Rate-Distortion Trade-off

$\alpha$ controls the trade-off between utterance complexity (rate) and information loss (distortion).

```{python}
contexts = [{'G': 0}, {'G': 1}]

rd_data = compute_rate_distortion_curve(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    contexts=contexts,
    alpha_range=np.logspace(-1, 2, 50)
)
```

```{python}
#| fig-cap: "Rate-distortion trade-off curve"
#| label: fig-rate-distortion

fig, axes = plt.subplots(1, 3, figsize=(14, 4))

ax = axes[0]
ax.plot(rd_data['rate'], rd_data['distortion'], 'b-', linewidth=2)
ax.set_xlabel('Rate (bits)')
ax.set_ylabel('Distortion (bits)')
ax.set_title('Rate-Distortion Trade-off')
ax.grid(True, alpha=0.3)

ax = axes[1]
ax.semilogx(rd_data['alpha'], rd_data['rate'], 'g-', linewidth=2)
ax.set_xlabel(r'$\alpha$ (rationality)')
ax.set_ylabel('Rate (bits)')
ax.set_title('Rate vs Rationality')
ax.grid(True, alpha=0.3)

ax = axes[2]
ax.semilogx(rd_data['alpha'], rd_data['distortion'], 'r-', linewidth=2)
ax.set_xlabel(r'$\alpha$ (rationality)')
ax.set_ylabel('Distortion (bits)')
ax.set_title('Distortion vs Rationality')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Credulous vs Vigilant Listener

Two listener types observe the same advice:

- **Credulous**: assumes $\psi = \text{informative}$, only infers world complexity
- **Vigilant**: jointly infers world complexity and speaker type $\psi \in \{\text{informative}, \text{persuade-up}, \text{persuade-down}, \text{unreliable}\}$

Speaker types:

| Type | Utility | World-dependent? |
|------|---------|-----------------|
| Informative | $-\text{KL}(P_\text{true} \| P_\text{compressed})$ | Yes |
| Persuade-up | $E[Y{=}1 \mid u, c]$ | No |
| Persuade-down | $E[Y{=}0 \mid u, c]$ | No |
| Unreliable | uniform random | No |

```{python}
scenario = build_trust_update_scenario()
world_dags = {'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']}
prior_range = np.linspace(0.05, 0.95, 40)

revision_obs = [
    ({'G': 1}, 'drug_works'),
    ({'G': 0}, 'drug_doesnt_work'),
]
consistent_obs = [
    ({'G': 1}, 'drug_works'),
    ({'G': 0}, 'drug_works'),
]

credulous_goal = {'informative': 1.0, 'persuade_up': 0.0, 'persuade_down': 0.0, 'unreliable': 0.0}
vigilant_goal = None  # uses default uniform prior over all 4 types
```

### Derived speaker likelihoods

Informative likelihoods are world-dependent; persuasive and unreliable likelihoods are not. Revision can only be explained by an informative speaker in a complex world.

```{python}
rsa_model = RSATrustModel(
    world_dags=world_dags,
    utterances=scenario['utterances'],
    effect_var='Y',
    prior_world={'simple': 0.5, 'complex': 0.5},
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)

print("P(u | world, goal, context):")
for ctx_label, ctx in [("G=0", {'G': 0}), ("G=1", {'G': 1})]:
    for utt in scenario['utterances']:
        lk = rsa_model.get_derived_likelihoods(ctx, utt.name)
        print(f"\n  P({utt.name} | {ctx_label}):")
        for (w, g), p in sorted(lk.items()):
            print(f"    {w:>8s}, {g:>14s}: {p:.4f}")
```

### Point comparison at flat prior

```{python}
def run_both_listeners(obs):
    results = {}
    for label, goal in [('Credulous', credulous_goal), ('Vigilant', vigilant_goal)]:
        m = RSATrustModel(
            world_dags=world_dags,
            utterances=scenario['utterances'],
            effect_var='Y',
            prior_world={'simple': 0.5, 'complex': 0.5},
            prior_goal=goal,
            speaker_alpha=10.0,
            contexts=scenario['contexts'],
        )
        results[label] = m.update(obs)
    return results

for obs_label, obs in [('REVISION', revision_obs), ('CONSISTENT', consistent_obs)]:
    print(f"\n{obs_label}:")
    results = run_both_listeners(obs)
    for label, r in results.items():
        trust_str = f"P(inf): {r['posterior_reliable']:.3f}" if label == 'Vigilant' else "P(inf): 1.000"
        print(f"  {label:10s}  P(complex): {r['posterior_complex']:.3f}   {trust_str}")
```

### Vigilance attenuates world inference

```{python}
#| fig-cap: "Credulous vs vigilant: world inference and trust"
#| label: fig-credulous-vs-vigilant

def sweep_listener(obs, prior_goal):
    complexity_deltas, trust_deltas = [], []
    for p in prior_range:
        m = RSATrustModel(
            world_dags=world_dags,
            utterances=scenario['utterances'],
            effect_var='Y',
            prior_world={'simple': 1-p, 'complex': float(p)},
            prior_goal=prior_goal,
            speaker_alpha=10.0,
            contexts=scenario['contexts'],
        )
        r = m.update(obs)
        complexity_deltas.append(r['complexity_delta'])
        trust_deltas.append(r['trust_delta'])
    return np.array(complexity_deltas), np.array(trust_deltas)

fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))

# Panel 1: Complexity delta — revision
ax = axes[0]
cd_cred_rev, _ = sweep_listener(revision_obs, credulous_goal)
cd_vig_rev, _ = sweep_listener(revision_obs, vigilant_goal)
ax.plot(prior_range, cd_cred_rev, 'r-', linewidth=2, label='Credulous')
ax.plot(prior_range, cd_vig_rev, 'r--', linewidth=2, label='Vigilant')
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)
ax.set_xlabel('P(complex) prior')
ax.set_ylabel(r'$\Delta$ P(complex)')
ax.set_title('World Inference: Revision')
ax.legend()
ax.grid(True, alpha=0.3)

# Panel 2: Complexity delta — consistent
ax = axes[1]
cd_cred_con, _ = sweep_listener(consistent_obs, credulous_goal)
cd_vig_con, _ = sweep_listener(consistent_obs, vigilant_goal)
ax.plot(prior_range, cd_cred_con, 'b-', linewidth=2, label='Credulous')
ax.plot(prior_range, cd_vig_con, 'b--', linewidth=2, label='Vigilant')
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)
ax.set_xlabel('P(complex) prior')
ax.set_ylabel(r'$\Delta$ P(complex)')
ax.set_title('World Inference: Consistent')
ax.legend()
ax.grid(True, alpha=0.3)

# Panel 3: Trust delta — vigilant only
ax = axes[2]
_, td_vig_rev = sweep_listener(revision_obs, vigilant_goal)
_, td_vig_con = sweep_listener(consistent_obs, vigilant_goal)
ax.plot(prior_range, td_vig_rev, 'r--', linewidth=2, label='Revision')
ax.plot(prior_range, td_vig_con, 'b--', linewidth=2, label='Consistent')
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)
ax.set_xlabel('P(complex) prior')
ax.set_ylabel(r'$\Delta$ P(informative)')
ax.set_title('Trust: Vigilant Only')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Trust Crossover

Whether advice helps or hurts trust depends on the complexity prior. Both observation types cross zero, but at different thresholds.

```{python}
#| fig-cap: "Trust crossover for the vigilant listener"
#| label: fig-trust-crossover

fig, ax = plt.subplots(figsize=(8, 5))

ax.plot(prior_range, td_vig_con, 'b-', linewidth=2.5, label='Consistent')
ax.plot(prior_range, td_vig_rev, 'r--', linewidth=2.5, label='Revision')
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)

for td, color, label in [(td_vig_con, 'blue', 'consistent'),
                          (td_vig_rev, 'red', 'revision')]:
    sign_changes = np.where(np.diff(np.sign(td)))[0]
    if len(sign_changes) > 0:
        idx = sign_changes[0]
        x0, x1 = prior_range[idx], prior_range[idx + 1]
        y0, y1 = td[idx], td[idx + 1]
        crossover = x0 - y0 * (x1 - x0) / (y1 - y0)
        ax.plot(crossover, 0, 'o', color=color, markersize=9)
        offset_y = 0.06 if label == 'revision' else -0.08
        ax.annotate(
            f'{label}: {crossover:.2f}',
            xy=(crossover, 0),
            xytext=(crossover + 0.08, offset_y),
            arrowprops=dict(arrowstyle='->', color=color),
            fontsize=10, color=color,
        )

ax.set_xlabel('P(world = complex)')
ax.set_ylabel(r'$\Delta$ P(speaker = informative)')
ax.set_title('When Does Compression Backfire?')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Effect of Explanation

Explanation shifts the listener's complexity prior upward before observing advice. For consistent advice this increases suspicion; for revision it reinforces the informative interpretation.

```{python}
#| fig-cap: "Explanation shifts trust curves"
#| label: fig-explanation-effect

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for ax, obs, title in [(axes[0], consistent_obs, 'Consistent'),
                        (axes[1], revision_obs, 'Revision')]:
    for strength, color, label in [(0, 'blue', 'No explanation'),
                                    (1.0, 'orange', 'Moderate'),
                                    (2.0, 'green', 'Strong')]:
        deltas = []
        for p_complex in prior_range:
            m = RSATrustModel(
                world_dags=world_dags,
                utterances=scenario['utterances'],
                effect_var='Y',
                prior_world={'simple': 1.0 - p_complex, 'complex': float(p_complex)},
                speaker_alpha=10.0,
                contexts=scenario['contexts'],
            )
            if strength == 0:
                r = m.update(obs)
            else:
                r = m.update_with_explanation(obs, explanation_strength=strength)
            deltas.append(r['trust_delta'])

        ax.plot(prior_range, deltas, color=color, linewidth=2, label=label)

    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax.set_xlabel('P(world = complex)')
    ax.set_ylabel(r'$\Delta$ P(informative)')
    ax.set_title(title)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Speaker Goal Inference

The vigilant listener also infers *why* the speaker said what they said.

```{python}
#| fig-cap: "Goal inference after revision vs consistent advice"
#| label: fig-goal-inference

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

for ax, obs, title in [(axes[0], revision_obs, 'After Revision'),
                        (axes[1], consistent_obs, 'After Consistent')]:
    goal_traces = {g: [] for g in ['informative', 'persuade_up', 'persuade_down', 'unreliable']}
    for p_complex in prior_range:
        m = RSATrustModel(
            world_dags=world_dags,
            utterances=scenario['utterances'],
            effect_var='Y',
            prior_world={'simple': 1.0 - p_complex, 'complex': float(p_complex)},
            speaker_alpha=10.0,
            contexts=scenario['contexts'],
        )
        m.update(obs)
        goals = m.get_goal_beliefs()
        for g in goal_traces:
            goal_traces[g].append(goals[g])

    ax.plot(prior_range, goal_traces['informative'], 'b-', linewidth=2, label='Informative')
    ax.plot(prior_range, goal_traces['persuade_up'], 'r-', linewidth=2, label='Persuade-up')
    ax.plot(prior_range, goal_traces['persuade_down'], 'g-', linewidth=2, label='Persuade-down')
    ax.plot(prior_range, goal_traces['unreliable'], 'k:', linewidth=2, label='Unreliable')
    ax.axhline(y=1/4, color='gray', linestyle='--', alpha=0.5, label='Prior')
    ax.set_xlabel('P(world = complex)')
    ax.set_ylabel('P(goal type)')
    ax.set_title(title)
    ax.legend(fontsize=9)
    ax.set_ylim(-0.05, 1.05)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```
