---
title: "Causal Compression Demonstrations"
format:
  html:
    code-fold: false
    toc: true
jupyter: python3
---

```{python}
import sys
sys.path.insert(0, '..')

import numpy as np
import matplotlib.pyplot as plt

from models import (
    Variable, CausalDAG, Utterance,
    CompressionSpeaker,
    RSATrustModel,
    compute_rate_distortion_curve,
    compute_trust_curve,
    build_drug_marker_scenario,
    build_trust_update_scenario,
)
```

## Context-Dependent Compression

A doctor advises patients about a drug treatment whose effectiveness depends on a genetic marker (G). G=1: drug is highly effective (90% success). G=0: drug is ineffective (20% success).

```{python}
true_dag, utterances = build_drug_marker_scenario()

speaker = CompressionSpeaker(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    alpha=10.0
)

print(f"Available utterances: {[u.name for u in utterances]}")
```

### Patient has the genetic marker

```{python}
context_marker = {'G': 1}
losses_marker = speaker.compute_losses(context_marker)
probs_marker = speaker.get_utterance_probs(context_marker)
optimal_marker = speaker.get_optimal_utterance(context_marker)

print("Context: G=1")
print("\nInformation losses (KL divergence):")
for u, l in losses_marker.items():
    print(f"  {u}: {l:.4f} bits")
print("\nUtterance probabilities:")
for u, p in probs_marker.items():
    print(f"  {u}: {p:.4f}")
print(f"\nOptimal utterance: '{optimal_marker.name}'")
```

### Patient lacks the genetic marker

```{python}
context_no_marker = {'G': 0}
losses_no_marker = speaker.compute_losses(context_no_marker)
probs_no_marker = speaker.get_utterance_probs(context_no_marker)
optimal_no_marker = speaker.get_optimal_utterance(context_no_marker)

print("Context: G=0")
print("\nInformation losses (KL divergence):")
for u, l in losses_no_marker.items():
    print(f"  {u}: {l:.4f} bits")
print("\nUtterance probabilities:")
for u, p in probs_no_marker.items():
    print(f"  {u}: {p:.4f}")
print(f"\nOptimal utterance: '{optimal_no_marker.name}'")
```

```{python}
#| fig-cap: "Utterance probabilities and information loss by context"
#| label: fig-context-compression

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

utterance_names = list(probs_marker.keys())
x = np.arange(len(utterance_names))
width = 0.35

ax = axes[0]
ax.bar(x - width/2, [probs_marker[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [probs_no_marker[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('P(utterance | context)')
ax.set_title('Utterance Probabilities by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

ax = axes[1]
losses_marker_finite = {k: v if v < 10 else 10 for k, v in losses_marker.items()}
losses_no_marker_finite = {k: v if v < 10 else 10 for k, v in losses_no_marker.items()}

ax.bar(x - width/2, [losses_marker_finite[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [losses_no_marker_finite[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('Information Loss (bits)')
ax.set_title('Information Loss by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Rate-Distortion Trade-off

The rationality parameter $\alpha$ controls the trade-off between utterance complexity (rate) and expected information loss (distortion). $\alpha \to 0$ gives a uniform distribution; $\alpha \to \infty$ gives deterministic selection of the optimal utterance.

```{python}
contexts = [{'G': 0}, {'G': 1}]

rd_data = compute_rate_distortion_curve(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    contexts=contexts,
    alpha_range=np.logspace(-1, 2, 50)
)

print(f"Rate range: [{rd_data['rate'].min():.3f}, {rd_data['rate'].max():.3f}] bits")
print(f"Distortion range: [{rd_data['distortion'].min():.3f}, {rd_data['distortion'].max():.3f}] bits")
```

```{python}
#| fig-cap: "Rate-distortion trade-off curve"
#| label: fig-rate-distortion

fig, axes = plt.subplots(1, 3, figsize=(14, 4))

ax = axes[0]
ax.plot(rd_data['rate'], rd_data['distortion'], 'b-', linewidth=2)
ax.set_xlabel('Rate (bits)')
ax.set_ylabel('Distortion (bits)')
ax.set_title('Rate-Distortion Trade-off')
ax.grid(True, alpha=0.3)

ax = axes[1]
ax.semilogx(rd_data['alpha'], rd_data['rate'], 'g-', linewidth=2)
ax.set_xlabel(r'$\alpha$ (rationality)')
ax.set_ylabel('Rate (bits)')
ax.set_title('Rate vs Rationality')
ax.grid(True, alpha=0.3)

ax = axes[2]
ax.semilogx(rd_data['alpha'], rd_data['distortion'], 'r-', linewidth=2)
ax.set_xlabel(r'$\alpha$ (rationality)')
ax.set_ylabel('Distortion (bits)')
ax.set_title('Distortion vs Rationality')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Persuasive RSA Trust Model

The `RSATrustModel` extends RSA to speakers with potentially persuasive goals. The listener jointly infers world complexity and speaker goal type $\psi \in \{\text{informative}, \text{persuade-up}, \text{persuade-down}\}$.

Speaker types:

- **Informative** ($\psi = \text{inf}$): minimizes information loss $P(u|w,c) \propto \exp[-\alpha \cdot \text{KL}(P_\text{true} \| P_\text{compressed})]$
- **Persuade-up** ($\psi = \text{pers+}$): inflates outcome belief $P(u|w,c) \propto \exp[\alpha \cdot E[Y{=}1|u,c]]$
- **Persuade-down** ($\psi = \text{pers-}$): deflates outcome belief $P(u|w,c) \propto \exp[\alpha \cdot E[Y{=}0|u,c]]$

```{python}
scenario = build_trust_update_scenario()

rsa_model = RSATrustModel(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var=scenario['effect_var'],
    prior_world={'simple': 0.5, 'complex': 0.5},
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)

print("Derived likelihoods P(u | world, goal, context):")
for ctx_label, ctx in [("G=0", {'G': 0}), ("G=1", {'G': 1})]:
    for utt in scenario['utterances']:
        lk = rsa_model.get_derived_likelihoods(ctx, utt.name)
        print(f"\n  P({utt.name} | context={ctx_label}):")
        for (w, g), p in sorted(lk.items()):
            print(f"    {w:>8s}, {g:>14s}: {p:.4f}")
```

### Revision signals (complex, informative)

When the speaker revises advice across contexts ("drug works" for G=1, then "drug doesn't work" for G=0), this is strong evidence the speaker is informative and the world is complex. No other (world, goal) combination predicts this pattern.

```{python}
revision_obs = [
    ({'G': 1}, 'drug_works'),
    ({'G': 0}, 'drug_doesnt_work'),
]

rsa_50 = RSATrustModel(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var='Y',
    prior_world={'simple': 0.5, 'complex': 0.5},
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)
result_rev = rsa_50.update(revision_obs)

print("After observing revision (drug_works -> drug_doesnt_work):")
print(f"  Trust (P(informative)): {result_rev['prior_reliable']:.3f} -> {result_rev['posterior_reliable']:.3f} (delta={result_rev['trust_delta']:+.3f})")
print(f"  Complexity: {result_rev['prior_complex']:.3f} -> {result_rev['posterior_complex']:.3f} (delta={result_rev['complexity_delta']:+.3f})")
print(f"\n  Joint beliefs:")
for (w, g), p in sorted(rsa_50.get_beliefs().items()):
    print(f"    {w:>8s}, {g:>14s}: {p:.4f}")
```

### Consistency is ambiguous

Consistent advice ("drug works" in both contexts) is compatible with (simple, informative) or (any world, persuade-up). Whether it increases or decreases trust depends on the listener's complexity prior.

```{python}
consistent_obs = [
    ({'G': 1}, 'drug_works'),
    ({'G': 0}, 'drug_works'),
]

# Simple believer
rsa_simple = RSATrustModel(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var='Y',
    prior_world={'simple': 0.9, 'complex': 0.1},
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)
r_simple = rsa_simple.update(consistent_obs)

# Complex believer
rsa_complex = RSATrustModel(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var='Y',
    prior_world={'simple': 0.1, 'complex': 0.9},
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)
r_complex = rsa_complex.update(consistent_obs)

print("After consistent advice (drug_works in both contexts):")
print(f"\nSimple believer (P(complex)=0.1):")
print(f"  Trust: {r_simple['prior_reliable']:.3f} -> {r_simple['posterior_reliable']:.3f} (delta={r_simple['trust_delta']:+.3f})")
print(f"\nComplex believer (P(complex)=0.9):")
print(f"  Trust: {r_complex['prior_reliable']:.3f} -> {r_complex['posterior_reliable']:.3f} (delta={r_complex['trust_delta']:+.3f})")
```

## Trust Crossover: Consistent Advice

Sweeping P(C=complex) from 0 to 1 with consistent observations reveals the crossover. Listeners who believe the world is simple trust consistent advice (the speaker is reporting the simple truth). Listeners who believe the world is complex distrust it (an informative speaker would have revised).

```{python}
prior_range = np.linspace(0.05, 0.95, 40)

trust_consistent = compute_trust_curve(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var='Y',
    observations=consistent_obs,
    prior_complex_range=prior_range,
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)

trust_revision = compute_trust_curve(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var='Y',
    observations=revision_obs,
    prior_complex_range=prior_range,
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)
```

```{python}
#| fig-cap: "Trust delta vs complexity prior for consistent and revision observations"
#| label: fig-trust-crossover

fig, ax = plt.subplots(figsize=(8, 5))

pc = trust_consistent['prior_complex']
td_cons = trust_consistent['trust_delta']
td_rev = trust_revision['trust_delta']

ax.plot(pc, td_cons, 'b-', linewidth=2, label='Consistent advice')
ax.plot(pc, td_rev, 'r--', linewidth=2, label='Revision')
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)

# Find and annotate crossover for consistent
sign_changes = np.where(np.diff(np.sign(td_cons)))[0]
if len(sign_changes) > 0:
    idx = sign_changes[0]
    x0, x1 = pc[idx], pc[idx + 1]
    y0, y1 = td_cons[idx], td_cons[idx + 1]
    crossover = x0 - y0 * (x1 - x0) / (y1 - y0)
    ax.plot(crossover, 0, 'bo', markersize=10)
    ax.annotate(
        f'crossover: P(complex)={crossover:.2f}',
        xy=(crossover, 0),
        xytext=(crossover + 0.12, -0.08),
        arrowprops=dict(arrowstyle='->', color='blue'),
        fontsize=10, color='blue',
    )

ax.set_xlabel('P(world = complex)', fontsize=12)
ax.set_ylabel(r'$\Delta$ P(speaker = informative)', fontsize=12)
ax.set_title('Trust Updates: Consistent vs Revision', fontsize=14)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Effect of Explanation

When the speaker provides an explanation (shifting the listener's prior toward "complex world"), the trust dynamics change. For consistent advice, explanation makes the listener more suspicious (they now expect revision). For revision, explanation reinforces the informative interpretation.

```{python}
#| fig-cap: "Explanation shifts trust curves"
#| label: fig-explanation-effect

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for ax, obs, title in [(axes[0], consistent_obs, 'Consistent Advice'),
                        (axes[1], revision_obs, 'Revision')]:
    for strength, color, label in [(0, 'blue', 'No explanation'),
                                    (1.0, 'orange', 'Moderate explanation'),
                                    (2.0, 'green', 'Strong explanation')]:
        deltas = []
        for p_complex in prior_range:
            m = RSATrustModel(
                world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
                utterances=scenario['utterances'],
                effect_var='Y',
                prior_world={'simple': 1.0 - p_complex, 'complex': float(p_complex)},
                speaker_alpha=10.0,
                contexts=scenario['contexts'],
            )
            if strength == 0:
                r = m.update(obs)
            else:
                r = m.update_with_explanation(obs, explanation_strength=strength)
            deltas.append(r['trust_delta'])

        ax.plot(prior_range, deltas, color=color, linewidth=2, label=label)

    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax.set_xlabel('P(world = complex)', fontsize=12)
    ax.set_ylabel(r'$\Delta$ P(informative)', fontsize=12)
    ax.set_title(title, fontsize=14)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Speaker Goal Inference

The model also infers the speaker's goal. After revision, the listener shifts strongly toward $\psi = \text{informative}$. After consistent positive advice, the listener sees a mixture of informative and persuade-up.

```{python}
#| fig-cap: "Goal inference after different observation patterns"
#| label: fig-goal-inference

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

for ax, obs, title in [(axes[0], revision_obs, 'After Revision'),
                        (axes[1], consistent_obs, 'After Consistent')]:
    inf_beliefs = []
    up_beliefs = []
    down_beliefs = []
    for p_complex in prior_range:
        m = RSATrustModel(
            world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
            utterances=scenario['utterances'],
            effect_var='Y',
            prior_world={'simple': 1.0 - p_complex, 'complex': float(p_complex)},
            speaker_alpha=10.0,
            contexts=scenario['contexts'],
        )
        m.update(obs)
        goals = m.get_goal_beliefs()
        inf_beliefs.append(goals['informative'])
        up_beliefs.append(goals['persuade_up'])
        down_beliefs.append(goals['persuade_down'])

    ax.plot(prior_range, inf_beliefs, 'b-', linewidth=2, label='Informative')
    ax.plot(prior_range, up_beliefs, 'r-', linewidth=2, label='Persuade-up')
    ax.plot(prior_range, down_beliefs, 'g-', linewidth=2, label='Persuade-down')
    ax.axhline(y=1/3, color='gray', linestyle='--', alpha=0.5, label='Prior')
    ax.set_xlabel('P(world = complex)', fontsize=12)
    ax.set_ylabel('P(goal type)', fontsize=12)
    ax.set_title(title, fontsize=14)
    ax.legend(fontsize=10)
    ax.set_ylim(-0.05, 1.05)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```
