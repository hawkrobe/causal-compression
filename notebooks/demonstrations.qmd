---
title: "Causal Compression Demonstrations"
format:
  html:
    code-fold: false
    toc: true
jupyter: python3
---

```{python}
import sys
sys.path.insert(0, '..')

import numpy as np
import matplotlib.pyplot as plt

from models import (
    Variable, CausalDAG, Utterance,
    CompressionSpeaker, TrustModel,
    RSATrustModel,
    compute_rate_distortion_curve,
    compute_trust_curve,
    build_drug_marker_scenario,
    build_trust_update_scenario,
)
```

## Context-Dependent Compression

A doctor advises patients about a drug treatment whose effectiveness depends on a genetic marker (G). G=1: drug is highly effective (90% success). G=0: drug is ineffective (20% success).

```{python}
true_dag, utterances = build_drug_marker_scenario()

speaker = CompressionSpeaker(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    alpha=10.0
)

print(f"Available utterances: {[u.name for u in utterances]}")
```

### Patient has the genetic marker

```{python}
context_marker = {'G': 1}
losses_marker = speaker.compute_losses(context_marker)
probs_marker = speaker.get_utterance_probs(context_marker)
optimal_marker = speaker.get_optimal_utterance(context_marker)

print("Context: G=1")
print("\nInformation losses (KL divergence):")
for u, l in losses_marker.items():
    print(f"  {u}: {l:.4f} bits")
print("\nUtterance probabilities:")
for u, p in probs_marker.items():
    print(f"  {u}: {p:.4f}")
print(f"\nOptimal utterance: '{optimal_marker.name}'")
```

### Patient lacks the genetic marker

```{python}
context_no_marker = {'G': 0}
losses_no_marker = speaker.compute_losses(context_no_marker)
probs_no_marker = speaker.get_utterance_probs(context_no_marker)
optimal_no_marker = speaker.get_optimal_utterance(context_no_marker)

print("Context: G=0")
print("\nInformation losses (KL divergence):")
for u, l in losses_no_marker.items():
    print(f"  {u}: {l:.4f} bits")
print("\nUtterance probabilities:")
for u, p in probs_no_marker.items():
    print(f"  {u}: {p:.4f}")
print(f"\nOptimal utterance: '{optimal_no_marker.name}'")
```

```{python}
#| fig-cap: "Utterance probabilities and information loss by context"
#| label: fig-context-compression

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

utterance_names = list(probs_marker.keys())
x = np.arange(len(utterance_names))
width = 0.35

ax = axes[0]
ax.bar(x - width/2, [probs_marker[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [probs_no_marker[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('P(utterance | context)')
ax.set_title('Utterance Probabilities by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

ax = axes[1]
losses_marker_finite = {k: v if v < 10 else 10 for k, v in losses_marker.items()}
losses_no_marker_finite = {k: v if v < 10 else 10 for k, v in losses_no_marker.items()}

ax.bar(x - width/2, [losses_marker_finite[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [losses_no_marker_finite[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('Information Loss (bits)')
ax.set_title('Information Loss by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Listener Trust Dynamics

Two listeners with different priors about world complexity observe the same advice revision (doctor says "drug works" to one patient, then "drug doesn't work" to another).

```{python}
listener_simple = TrustModel.create_simple_believer(reliability_prior=0.8)
listener_complex = TrustModel.create_complex_believer(reliability_prior=0.8)

print("Listener A (Simple Believer): P(world=simple) = 0.9")
print("Listener B (Complex Believer): P(world=complex) = 0.9")
print(f"Both start with P(speaker=reliable) = 0.80")
```

### Trust updates after revision

```{python}
result_simple = listener_simple.update('revision')
result_complex = listener_complex.update('revision')

print("Listener A (simple believer):")
print(f"  Trust: {result_simple['prior_reliable']:.2f} -> {result_simple['posterior_reliable']:.3f} ({result_simple['trust_delta']:+.3f})")

print("\nListener B (complex believer):")
print(f"  Trust: {result_complex['prior_reliable']:.2f} -> {result_complex['posterior_reliable']:.3f} ({result_complex['trust_delta']:+.3f})")
```

```{python}
#| fig-cap: "Trust updates after observing advice revision"
#| label: fig-trust-dynamics

fig, ax = plt.subplots(figsize=(8, 5))

labels = ['Simple Believer', 'Complex Believer']
prior_trust = [result_simple['prior_reliable'], result_complex['prior_reliable']]
posterior_trust = [result_simple['posterior_reliable'], result_complex['posterior_reliable']]

x = np.arange(len(labels))
width = 0.35

bars1 = ax.bar(x - width/2, prior_trust, width, label='Before Revision',
               color='lightblue', edgecolor='steelblue')
bars2 = ax.bar(x + width/2, posterior_trust, width, label='After Revision',
               color='steelblue', edgecolor='darkblue')

for i, (p1, p2) in enumerate(zip(prior_trust, posterior_trust)):
    mid_x = i
    color = 'red' if p2 < p1 else 'green'
    arrow_text = f"{p2-p1:+.3f}"
    ax.annotate(arrow_text, xy=(mid_x, max(p1, p2) + 0.05), ha='center', fontsize=12,
                color=color, fontweight='bold')

ax.set_ylabel('P(speaker is reliable)', fontsize=12)
ax.set_title('Trust Updates After Observing Advice Revision', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(labels, fontsize=11)
ax.legend(loc='lower right')
ax.set_ylim(0, 1.1)
ax.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Rate-Distortion Trade-off

The rationality parameter $\alpha$ controls the trade-off between utterance complexity (rate) and expected information loss (distortion). $\alpha \to 0$ gives a uniform distribution; $\alpha \to \infty$ gives deterministic selection of the optimal utterance.

```{python}
contexts = [{'G': 0}, {'G': 1}]

rd_data = compute_rate_distortion_curve(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    contexts=contexts,
    alpha_range=np.logspace(-1, 2, 50)
)

print(f"Rate range: [{rd_data['rate'].min():.3f}, {rd_data['rate'].max():.3f}] bits")
print(f"Distortion range: [{rd_data['distortion'].min():.3f}, {rd_data['distortion'].max():.3f}] bits")
```

```{python}
#| fig-cap: "Rate-distortion trade-off curve"
#| label: fig-rate-distortion

fig, axes = plt.subplots(1, 3, figsize=(14, 4))

ax = axes[0]
ax.plot(rd_data['rate'], rd_data['distortion'], 'b-', linewidth=2)
ax.set_xlabel('Rate (bits)')
ax.set_ylabel('Distortion (bits)')
ax.set_title('Rate-Distortion Trade-off')
ax.grid(True, alpha=0.3)

ax = axes[1]
ax.semilogx(rd_data['alpha'], rd_data['rate'], 'g-', linewidth=2)
ax.set_xlabel(r'$\alpha$ (rationality)')
ax.set_ylabel('Rate (bits)')
ax.set_title('Rate vs Rationality')
ax.grid(True, alpha=0.3)

ax = axes[2]
ax.semilogx(rd_data['alpha'], rd_data['distortion'], 'r-', linewidth=2)
ax.set_xlabel(r'$\alpha$ (rationality)')
ax.set_ylabel('Distortion (bits)')
ax.set_title('Distortion vs Rationality')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## RSA-Derived Trust Updates

The `RSATrustModel` derives speaker likelihoods from `CompressionSpeaker` rather than hand-coding them. The listener jointly infers world complexity and speaker reliability via the memo RSA model.

```{python}
scenario = build_trust_update_scenario()

rsa_model = RSATrustModel(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var=scenario['effect_var'],
    prior_world={'simple': 0.5, 'complex': 0.5},
    prior_reliable=0.8,
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)

print("Derived likelihoods P(u | world, reliability, context):")
for ctx_label, ctx in [("G=0", {'G': 0}), ("G=1", {'G': 1})]:
    for utt in scenario['utterances']:
        lk = rsa_model.get_derived_likelihoods(ctx, utt.name)
        print(f"\n  P({utt.name} | context={ctx_label}):")
        for (w, r), p in sorted(lk.items()):
            print(f"    {w:>8s}, {r:>12s}: {p:.4f}")
```

### Opposite trust updates for simple vs complex believers

```{python}
observations = [
    ({'G': 1}, 'drug_works'),
    ({'G': 0}, 'drug_doesnt_work'),
]

# Simple believer
rsa_simple = RSATrustModel(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var='Y',
    prior_world={'simple': 0.9, 'complex': 0.1},
    prior_reliable=0.8,
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)
result_rsa_simple = rsa_simple.update(observations)

# Complex believer
rsa_complex = RSATrustModel(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var='Y',
    prior_world={'simple': 0.1, 'complex': 0.9},
    prior_reliable=0.8,
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)
result_rsa_complex = rsa_complex.update(observations)

print("RSA-derived trust updates after context-dependent revision:")
print(f"\nSimple believer (P(complex)=0.1):")
print(f"  Trust: {result_rsa_simple['prior_reliable']:.3f} -> {result_rsa_simple['posterior_reliable']:.3f} (delta={result_rsa_simple['trust_delta']:+.3f})")
print(f"  Complexity: {result_rsa_simple['prior_complex']:.3f} -> {result_rsa_simple['posterior_complex']:.3f}")

print(f"\nComplex believer (P(complex)=0.9):")
print(f"  Trust: {result_rsa_complex['prior_reliable']:.3f} -> {result_rsa_complex['posterior_reliable']:.3f} (delta={result_rsa_complex['trust_delta']:+.3f})")
print(f"  Complexity: {result_rsa_complex['prior_complex']:.3f} -> {result_rsa_complex['posterior_complex']:.3f}")
```

## Trust as a Function of Complexity Prior

Sweeping P(C=complex) from 0 to 1 reveals a crossover: listeners who believe the world is simple lose trust after revision, while listeners who believe the world is complex gain trust.

```{python}
prior_range = np.linspace(0.05, 0.95, 40)

trust_data = compute_trust_curve(
    world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
    utterances=scenario['utterances'],
    effect_var='Y',
    observations=observations,
    prior_complex_range=prior_range,
    prior_reliable=0.8,
    speaker_alpha=10.0,
    contexts=scenario['contexts'],
)
```

```{python}
#| fig-cap: "Trust delta vs complexity prior â€” the signature crossover"
#| label: fig-trust-crossover

fig, ax = plt.subplots(figsize=(8, 5))

pc = trust_data['prior_complex']
td = trust_data['trust_delta']

ax.plot(pc, td, 'b-', linewidth=2)
ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)

# Find and annotate crossover
sign_changes = np.where(np.diff(np.sign(td)))[0]
if len(sign_changes) > 0:
    idx = sign_changes[0]
    x0, x1 = pc[idx], pc[idx + 1]
    y0, y1 = td[idx], td[idx + 1]
    crossover = x0 - y0 * (x1 - x0) / (y1 - y0)
    ax.plot(crossover, 0, 'ro', markersize=10)
    ax.annotate(
        f'crossover: P(complex)={crossover:.2f}',
        xy=(crossover, 0),
        xytext=(crossover + 0.1, max(td) * 0.3),
        arrowprops=dict(arrowstyle='->', color='red'),
        fontsize=10, color='red',
    )

ax.set_xlabel('P(world = complex)', fontsize=12)
ax.set_ylabel('Trust delta after revision', fontsize=12)
ax.set_title('RSA-Derived Trust Predictions', fontsize=14)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Effect of Explanation

When the speaker provides an explanation (shifting the listener's prior toward "complex world"), the trust-damaging effect of revision is reduced. We model explanation as a log-odds shift toward P(C=complex) before the Bayesian update.

```{python}
#| fig-cap: "Explanation shifts the trust curve upward"
#| label: fig-explanation-effect

fig, ax = plt.subplots(figsize=(8, 5))

for strength, color, label in [(0, 'blue', 'No explanation'),
                                (1.0, 'orange', 'Moderate explanation'),
                                (2.0, 'green', 'Strong explanation')]:
    deltas = []
    for p_complex in prior_range:
        m = RSATrustModel(
            world_dags={'simple': scenario['simple_dag'], 'complex': scenario['complex_dag']},
            utterances=scenario['utterances'],
            effect_var='Y',
            prior_world={'simple': 1.0 - p_complex, 'complex': float(p_complex)},
            prior_reliable=0.8,
            speaker_alpha=10.0,
            contexts=scenario['contexts'],
        )
        if strength == 0:
            r = m.update(observations)
        else:
            r = m.update_with_explanation(observations, explanation_strength=strength)
        deltas.append(r['trust_delta'])

    ax.plot(prior_range, deltas, color=color, linewidth=2, label=label)

ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('P(world = complex)', fontsize=12)
ax.set_ylabel('Trust delta after revision', fontsize=12)
ax.set_title('Effect of Explanation on Trust', fontsize=14)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Comparison: Derived vs Hand-Coded Likelihoods

The hand-coded `TrustModel` uses stipulated likelihoods. The `RSATrustModel` derives them from the speaker's compression behavior. Here we overlay both to show they agree qualitatively but differ in magnitude.

```{python}
#| fig-cap: "RSA-derived vs hand-coded trust models"
#| label: fig-derived-vs-handcoded

fig, ax = plt.subplots(figsize=(8, 5))

# RSA-derived curve (already computed)
ax.plot(trust_data['prior_complex'], trust_data['trust_delta'],
        'b-', linewidth=2, label='RSA-derived')

# Hand-coded curve
hc_deltas = []
for p_complex in prior_range:
    tm = TrustModel(
        prior_world_complex=float(p_complex),
        prior_reliable=0.8,
    )
    r = tm.update('revision')
    hc_deltas.append(r['trust_delta'])

ax.plot(prior_range, hc_deltas, 'r--', linewidth=2, label='Hand-coded')

ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
ax.set_xlabel('P(world = complex)', fontsize=12)
ax.set_ylabel('Trust delta after revision', fontsize=12)
ax.set_title('Derived vs Hand-Coded Trust Models', fontsize=14)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```
