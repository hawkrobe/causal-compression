---
title: "Causal Compression Demonstrations"
format:
  html:
    code-fold: false
    toc: true
jupyter: python3
---

```{python}
import sys
sys.path.insert(0, '../src')

import numpy as np
import matplotlib.pyplot as plt

from causal_compression import (
    Variable, CausalDAG, Utterance,
    CompressionSpeaker, TrustModel,
    compute_rate_distortion_curve, plot_rate_distortion_curve,
    build_drug_marker_scenario
)
```

## Context-Dependent Compression

A doctor advises patients about a drug treatment whose effectiveness depends on a genetic marker (G). G=1: drug is highly effective (90% success). G=0: drug is ineffective (20% success).

```{python}
true_dag, utterances = build_drug_marker_scenario()

speaker = CompressionSpeaker(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    alpha=10.0
)

print(f"Available utterances: {[u.name for u in utterances]}")
```

### Patient has the genetic marker

```{python}
context_marker = {'G': 1}
losses_marker = speaker.compute_losses(context_marker)
probs_marker = speaker.get_utterance_probs(context_marker)
optimal_marker = speaker.get_optimal_utterance(context_marker)

print("Context: G=1")
print("\nInformation losses (KL divergence):")
for u, l in losses_marker.items():
    print(f"  {u}: {l:.4f} bits")
print("\nUtterance probabilities:")
for u, p in probs_marker.items():
    print(f"  {u}: {p:.4f}")
print(f"\nOptimal utterance: '{optimal_marker.name}'")
```

### Patient lacks the genetic marker

```{python}
context_no_marker = {'G': 0}
losses_no_marker = speaker.compute_losses(context_no_marker)
probs_no_marker = speaker.get_utterance_probs(context_no_marker)
optimal_no_marker = speaker.get_optimal_utterance(context_no_marker)

print("Context: G=0")
print("\nInformation losses (KL divergence):")
for u, l in losses_no_marker.items():
    print(f"  {u}: {l:.4f} bits")
print("\nUtterance probabilities:")
for u, p in probs_no_marker.items():
    print(f"  {u}: {p:.4f}")
print(f"\nOptimal utterance: '{optimal_no_marker.name}'")
```

```{python}
#| fig-cap: "Utterance probabilities and information loss by context"
#| label: fig-context-compression

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

utterance_names = list(probs_marker.keys())
x = np.arange(len(utterance_names))
width = 0.35

ax = axes[0]
ax.bar(x - width/2, [probs_marker[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [probs_no_marker[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('P(utterance | context)')
ax.set_title('Utterance Probabilities by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

ax = axes[1]
losses_marker_finite = {k: v if v < 10 else 10 for k, v in losses_marker.items()}
losses_no_marker_finite = {k: v if v < 10 else 10 for k, v in losses_no_marker.items()}

ax.bar(x - width/2, [losses_marker_finite[u] for u in utterance_names], width,
       label='G=1 (has marker)', color='steelblue')
ax.bar(x + width/2, [losses_no_marker_finite[u] for u in utterance_names], width,
       label='G=0 (no marker)', color='coral')
ax.set_ylabel('Information Loss (bits)')
ax.set_title('Information Loss by Context')
ax.set_xticks(x)
ax.set_xticklabels(utterance_names, rotation=15)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Listener Trust Dynamics

Two listeners with different priors about world complexity observe the same advice revision (doctor says "drug works" to one patient, then "drug doesn't work" to another).

```{python}
listener_simple = TrustModel.create_simple_believer(reliability_prior=0.8)
listener_complex = TrustModel.create_complex_believer(reliability_prior=0.8)

print("Listener A (Simple Believer): P(world=simple) = 0.9")
print("Listener B (Complex Believer): P(world=complex) = 0.9")
print(f"Both start with P(speaker=reliable) = 0.80")
```

### Trust updates after revision

```{python}
result_simple = listener_simple.update('revision')
result_complex = listener_complex.update('revision')

print("Listener A (simple believer):")
print(f"  Trust: {result_simple['prior_reliable']:.2f} -> {result_simple['posterior_reliable']:.3f} ({result_simple['trust_delta']:+.3f})")

print("\nListener B (complex believer):")
print(f"  Trust: {result_complex['prior_reliable']:.2f} -> {result_complex['posterior_reliable']:.3f} ({result_complex['trust_delta']:+.3f})")
```

```{python}
#| fig-cap: "Trust updates after observing advice revision"
#| label: fig-trust-dynamics

fig, ax = plt.subplots(figsize=(8, 5))

labels = ['Simple Believer', 'Complex Believer']
prior_trust = [result_simple['prior_reliable'], result_complex['prior_reliable']]
posterior_trust = [result_simple['posterior_reliable'], result_complex['posterior_reliable']]

x = np.arange(len(labels))
width = 0.35

bars1 = ax.bar(x - width/2, prior_trust, width, label='Before Revision',
               color='lightblue', edgecolor='steelblue')
bars2 = ax.bar(x + width/2, posterior_trust, width, label='After Revision',
               color='steelblue', edgecolor='darkblue')

for i, (p1, p2) in enumerate(zip(prior_trust, posterior_trust)):
    mid_x = i
    color = 'red' if p2 < p1 else 'green'
    arrow_text = f"{p2-p1:+.3f}"
    ax.annotate(arrow_text, xy=(mid_x, max(p1, p2) + 0.05), ha='center', fontsize=12,
                color=color, fontweight='bold')

ax.set_ylabel('P(speaker is reliable)', fontsize=12)
ax.set_title('Trust Updates After Observing Advice Revision', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(labels, fontsize=11)
ax.legend(loc='lower right')
ax.set_ylim(0, 1.1)
ax.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Rate-Distortion Trade-off

The rationality parameter $\alpha$ controls the trade-off between utterance complexity (rate) and expected information loss (distortion). $\alpha \to 0$ gives a uniform distribution; $\alpha \to \infty$ gives deterministic selection of the optimal utterance.

```{python}
contexts = [{'G': 0}, {'G': 1}]

rd_data = compute_rate_distortion_curve(
    true_dag=true_dag,
    utterances=utterances,
    effect_var='Y',
    contexts=contexts,
    alpha_range=np.logspace(-1, 2, 50)
)

print(f"Rate range: [{rd_data['rate'].min():.3f}, {rd_data['rate'].max():.3f}] bits")
print(f"Distortion range: [{rd_data['distortion'].min():.3f}, {rd_data['distortion'].max():.3f}] bits")
```

```{python}
#| fig-cap: "Rate-distortion trade-off curve"
#| label: fig-rate-distortion

fig, axes = plot_rate_distortion_curve(rd_data, figsize=(14, 4))
plt.show()
```
